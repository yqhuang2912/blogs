<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>生成模型系列（一）：从 VAE 出发 | 慢变量</title>
    <link rel="stylesheet" href="../styles.css">
    <link rel="icon" href="../icons/deceleration.png" type="image/png">

    <script type="application/json" id="post-metadata">
        {
            "id": "11398",
            "slug": "generative_model_vae",
            "title": "生成模型系列（一）：从 VAE 出发",
            "createdAt": "2025-11-23",
            "day": "23",
            "month": "Nov",
            "categories": [
                "人工智能"
            ],
            "tags": [
                "生成模型",
                "概率图模型",
                "VAE"
            ],
            "summary": [],
            "link": "posts/generative_model_vae.html"
        }
    </script>

    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>

<body>
    <div class="page-container">
        <div data-include="../partials/navbar.html"></div>

    <main class="main-content">
        <div class="wrapper">
            <article class="post">
                <div data-component="post-header"
                     data-day="23"
                     data-month="Nov"
                     data-title="生成模型系列（一）：从 VAE 出发"
                     data-link=""
                     data-meta="&lt;span class=&quot;meta-item meta-categories&quot;&gt;分类：&lt;a href=&quot;#&quot;&gt;人工智能&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;meta-divider&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;meta-item meta-tags&quot;&gt;标签：&lt;a href=&quot;#&quot;&gt;生成模型&lt;/a&gt;,&lt;a href=&quot;#&quot;&gt;概率图模型&lt;/a&gt;,&lt;a href=&quot;#&quot;&gt;VAE&lt;/a&gt;&lt;/span&gt;"
                     data-heading="h1"
                     data-meta-class="post-meta"
                     data-title-class="post-title"></div>

                <div class="post-content single-post-content">
                <section class="post-section" id="判别模型-vs-生成模型：我们到底想学什么？"><h2 data-heading-id="判别模型-vs-生成模型：我们到底想学什么？">判别模型 vs 生成模型：我们到底想学什么？</h2>
                <p>在监督学习里，我们最熟的是<strong>判别模型（discriminative model）</strong>，比如：</p>
                <ul>
                <li>给定一张CT图像$x$，预测它是否有病灶$y$：学的是$p(y \mid x)$  </li>
                <li>给定一段文本，预测情感标签：学的是$p(\text{label} \mid \text{text})$</li>
                </ul>
                <p>这类模型关心的是「<strong>输出标签</strong>」，不关心数据本身是怎么「长出来的」。而<strong>生成模型（generative model）</strong>，则试图学的是<strong>数据本身的分布</strong> $p(x)$，甚至：</p>
                <ul>
                <li>学习联合分布$p(x, y)$，从而既能做生成，也能做分类；  </li>
                <li>或者学习一个条件分布$p(x \mid c)$，可以在条件$c$下生成样本（条件生成）。</li>
                </ul>
                </section><section class="post-section" id="几种主流的生成建模路径"><h2 data-heading-id="几种主流的生成建模路径">几种主流的生成建模路径</h2>
                <p>粗略按「是否有显式的概率密度」划分，可以把流行的生成模型分成几类：</p>
                <ol>
                <li><p><strong>显式密度模型（explicit density）</strong></p>
                <ul>
                <li>直接建模 $p_\theta(x)$ 或者 $p_\theta(x\mid z)$  </li>
                <li>通常配合最大似然训练（或其近似）</li>
                <li>代表：<ul>
                <li>自回归模型（PixelCNN、GPT 等）</li>
                <li>变分自编码器（VAE）：$p_\theta(x, z) = p(z)p_\theta(x\mid z)$</li>
                </ul>
                </li>
                </ul>
                </li>
                <li><p><strong>正则化变换模型（normalizing flows）</strong></p>
                <ul>
                <li>学一个可逆变换 $x = f_\theta(z)$，$z$ 服从简单分布（如高斯）</li>
                <li>通过变换的雅可比行列式精确算出 $p_\theta(x)$</li>
                <li>代表：RealNVP、Glow 等</li>
                </ul>
                </li>
                <li><p><strong>隐式生成模型（implicit model）</strong></p>
                <ul>
                <li>没有显式的 $p_\theta(x)$ 公式，只能通过采样间接定义分布</li>
                <li>多通过对抗训练、得分匹配、扩散过程来逼近数据分布</li>
                <li>代表：GAN、Diffusion Models</li>
                </ul>
                </li>
                </ol>
                <p><strong>VAE 属于第一类</strong>：  </p>
                <blockquote>
                <p>它假设存在一个潜变量 $z$，通过 $p_\theta(x\mid z)$ 来生成数据，<br>再用「变分推断」去近似 $p_\theta(z\mid x)$。</p>
                </blockquote>
                <hr>
                </section><section class="post-section" id="3-从概率建模视角看-vae：引入潜变量"><h2 data-heading-id="3-从概率建模视角看-vae：引入潜变量">3. 从概率建模视角看 VAE：引入潜变量</h2>
                <h3 id="31-为何引入潜变量-math41？">3.1 为何引入潜变量 $z$？</h3>
                <p>直接建模高维数据（比如 $512\times512$ 的医疗图像）的 $p(x)$ 通常非常难：</p>
                <ul>
                <li>维度极高；</li>
                <li>数据分布结构复杂（多模态、强非线性）；</li>
                <li>很难直接写出一个参数化的、可积的 $p_\theta(x)$。</li>
                </ul>
                <p>一个常见的思路是：<strong>引入低维潜变量 $z$</strong>：</p>
                <ul>
                <li><p>$z$ 可以理解为“生成这张图像时的一些隐含因素”：  </p>
                <ul>
                <li>在自然图像中：姿态、光照、类别、风格等；  </li>
                <li>在医学图像中：器官结构、病灶类型、成像条件等。</li>
                </ul>
                </li>
                <li><p>然后假设数据是「先采 $z$ 再生成 $x$」：</p>
                <div class="math-block">$$
                  z \sim p(z),\qquad
                  x \sim p_\theta(x \mid z).
                  $$</div>
                </li>
                </ul>
                <p>组合起来，得到联合分布：</p>
                <div class="math-block">$$
                p_\theta(x, z) = p(z)\, p_\theta(x \mid z). \tag{3.1}
                $$</div>
                <p>边缘分布（对 $z$ 积分）就是我们真正想学的 $p_\theta(x)$：</p>
                <div class="math-block">$$
                p_\theta(x) = \int p_\theta(x, z)\, dz = \int p(z)\, p_\theta(x\mid z)\, dz. \tag{3.2}
                $$</div>
                <blockquote>
                <p><strong>直观理解</strong>：<br>我们假设「潜空间」里一切比较简单，<br>把「复杂的高维数据」看成是潜空间里简单分布经过一个复杂非线性网络（decoder）“推出来”的结果。</p>
                </blockquote>
                <hr>
                <h3 id="32-生成模型的训练目标：最大似然">3.2 生成模型的训练目标：最大似然</h3>
                <p>假设有数据集 $\{x^{(i)}\}_{i=1}^N$，最朴素的思想是用**最大似然估计（MLE）**来训练：</p>
                <div class="math-block">$$
                \max_\theta \sum_{i=1}^N \log p_\theta(x^{(i)}). \tag{3.3}
                $$</div>
                <p>带入（3.2）：</p>
                <div class="math-block">$$
                \log p_\theta(x^{(i)})
                = \log \int p(z)\, p_\theta(x^{(i)} \mid z)\, dz. \tag{3.4}
                $$</div>
                <p>问题来了：  </p>
                <blockquote>
                <p>这个积分通常<strong>既高维又没有解析解</strong>，而且里面有非线性神经网络，很难直接算出 $\log p_\theta(x)$。</p>
                </blockquote>
                <p>于是，我们需要<strong>两个东西</strong>：</p>
                <ol>
                <li>一种方式来近似这个积分 / 对数似然；</li>
                <li>一种方式来近似后验 $p_\theta(z\mid x)$，因为它也会经常出现。</li>
                </ol>
                <p>这就自然把我们带到了：<strong>变分推断（variational inference）</strong>。</p>
                <hr>
                </section><section class="post-section" id="4-后验推断的难点：为什么要「变分」？"><h2 data-heading-id="4-后验推断的难点：为什么要「变分」？">4. 后验推断的难点：为什么要「变分」？</h2>
                <p>在潜变量模型中，贝叶斯后验是：</p>
                <div class="math-block">$$
                p_\theta(z \mid x)
                = \frac{p_\theta(x, z)}{p_\theta(x)}
                = \frac{p(z)p_\theta(x\mid z)}{\int p(z')p_\theta(x\mid z')\, dz'}. \tag{4.1}
                $$</div>
                <ul>
                <li>分子好算（prior + likelihood）；</li>
                <li>分母是我们刚才说的那坨难算的积分 $p_\theta(x)$。</li>
                </ul>
                <p>这意味着：</p>
                <ul>
                <li>后验 $p_\theta(z\mid x)$ 一般<strong>没有解析形式</strong>；</li>
                <li>连归一化常数都不知道，很难直接从中采样或计算期望。</li>
                </ul>
                <blockquote>
                <p>但 VAE 又非常想要这个后验——<br>因为我们希望通过「编码器」把 $x$ 映射到潜空间 $z$，这个过程本质上就是在近似 $p_\theta(z\mid x)$。</p>
                </blockquote>
                <p>于是，VAE 做了一件经典的事：  </p>
                <blockquote>
                <p>用一个<strong>可控、可微、参数化的网络</strong> $q_\phi(z\mid x)$，去近似真实后验 $p_\theta(z\mid x)$。<br>这就是「变分分布」或「变分后验」。</p>
                </blockquote>
                <hr>
                </section><section class="post-section" id="5-vae-的核心：从-log-p-到变分下界（elbo）"><h2 data-heading-id="5-vae-的核心：从-log-p-到变分下界（elbo）">5. VAE 的核心：从 log p 到变分下界（ELBO）</h2>
                <p>接下来是 VAE 中最重要、也是最经典的推导步骤：<br><strong>把 $\log p_\theta(x)$ 写成一个「下界 + KL」的形式</strong>。</p>
                <h3 id="51-插入变分分布-math62">5.1 插入变分分布 $q_\phi(z\mid x)$</h3>
                <p>从边缘似然开始：</p>
                <div class="math-block">$$
                \log p_\theta(x)
                = \log \int p_\theta(x, z)\, dz. \tag{5.1}
                $$</div>
                <p>乘上 1（“乘除同一个东西”）：</p>
                <div class="math-block">$$
                \begin{aligned}
                \log p_\theta(x)
                &amp;= \log \int p_\theta(x, z)\, dz \\
                &amp;= \log \int 
                \frac{p_\theta(x, z)}{q_\phi(z\mid x)}\,
                q_\phi(z\mid x)\, dz. \tag{5.2}
                \end{aligned}
                $$</div>
                <p>把这个积分解释为在 $q_\phi(z\mid x)$ 下的期望：</p>
                <div class="math-block">$$
                \log p_\theta(x)
                = \log \mathbb{E}_{z\sim q_\phi(z\mid x)}
                \left[
                  \frac{p_\theta(x, z)}{q_\phi(z\mid x)}
                \right]. \tag{5.3}
                $$</div>
                <h3 id="52-用-jensen-不等式得到下界（elbo）">5.2 用 Jensen 不等式得到下界（ELBO）</h3>
                <p>对 $\log(\cdot)$ 使用 Jensen 不等式：</p>
                <div class="math-block">$$
                \log \mathbb{E}[Y] \ge \mathbb{E}[\log Y].
                $$</div>
                <p>令
                $$
                Y = \frac{p_\theta(x, z)}{q_\phi(z\mid x)},
                $$
                得到：</p>
                <div class="math-block">$$
                \begin{aligned}
                \log p_\theta(x)
                &amp;= \log \mathbb{E}_{q_\phi}
                \left[
                  \frac{p_\theta(x, z)}{q_\phi(z\mid x)}
                \right] \\
                &amp;\ge \mathbb{E}_{z\sim q_\phi(z\mid x)}
                \left[
                  \log \frac{p_\theta(x, z)}{q_\phi(z\mid x)}
                \right]. \tag{5.4}
                \end{aligned}
                $$</div>
                <p>记右边为：</p>
                <div class="math-block">$$
                \mathcal{L}(\theta,\phi;x)
                := \mathbb{E}_{z\sim q_\phi(z\mid x)}
                \left[
                  \log \frac{p_\theta(x, z)}{q_\phi(z\mid x)}
                \right]. \tag{5.5}
                $$</div>
                <p>这就是著名的 <strong>变分下界（Evidence Lower BOund, ELBO）</strong>：</p>
                <div class="math-block">$$
                \boxed{
                \log p_\theta(x) \ge \mathcal{L}(\theta,\phi;x)
                }
                $$</div>
                <p>VAE 的训练目标就是：  </p>
                <blockquote>
                <p>选择 $\theta,\phi$，让 <strong>ELBO 尽可能大</strong>。<br>ELBO 越大，说明：</p>
                <ul>
                <li>一方面 $p_\theta(x)$ 越大（模型越能解释数据），  </li>
                <li>另一方面变分后验 $q_\phi(z\mid x)$ 越接近真实后验。</li>
                </ul>
                </blockquote>
                <hr>
                <h3 id="53-把-elbo-拆成「重构项-kl-项」">5.3 把 ELBO 拆成「重构项 - KL 项」</h3>
                <p>对 $\mathcal{L}(\theta,\phi;x)$ 做一点代数处理：</p>
                <p>先把 $p_\theta(x,z)$ 展开：</p>
                <div class="math-block">$$
                p_\theta(x,z) = p(z) p_\theta(x\mid z). \tag{5.6}
                $$</div>
                <p>代入 (5.5)：</p>
                <div class="math-block">$$
                \begin{aligned}
                \mathcal{L}(\theta,\phi;x)
                &amp;= \mathbb{E}_{q_\phi}
                \left[
                  \log p(z) + \log p_\theta(x\mid z)
                  - \log q_\phi(z\mid x)
                \right] \\
                &amp;= \mathbb{E}_{q_\phi}
                    \big[\log p_\theta(x\mid z)\big]
                  + \mathbb{E}_{q_\phi}
                    \big[\log p(z) - \log q_\phi(z\mid x)\big]. \tag{5.7}
                \end{aligned}
                $$</div>
                <p>注意第二项：</p>
                <div class="math-block">$$
                \mathbb{E}_{q_\phi}
                \big[\log p(z) - \log q_\phi(z\mid x)\big]
                = - \mathbb{E}_{q_\phi}
                \left[
                  \log \frac{q_\phi(z\mid x)}{p(z)}
                \right]
                = - \mathrm{KL}\big(q_\phi(z\mid x)\,\|\,p(z)\big). \tag{5.8}
                $$</div>
                <p>于是 ELBO 可以写成非常常见的形式：</p>
                <div class="math-block">$$
                \boxed{
                \mathcal{L}(\theta,\phi;x)
                =
                \underbrace{
                \mathbb{E}_{z\sim q_\phi(z\mid x)}
                  \big[\log p_\theta(x\mid z)\big]
                }_{\text{重构项}}
                -
                \underbrace{
                \mathrm{KL}\big(q_\phi(z\mid x)\,\|\,p(z)\big)
                }_{\text{正则项：变分后验逼近先验}}
                } \tag{5.9}
                $$</div>
                <p>这就是我们在 VAE 代码里常见的 loss 结构：</p>
                <ul>
                <li><strong>重构 loss</strong>：让 decoder 在采样的 $z$ 上尽量把 $x$ 复原出来；</li>
                <li><strong>KL loss</strong>：鼓励 $q_\phi(z\mid x)$ 不要偏离 prior 太远，避免潜空间乱跑。</li>
                </ul>
                <hr>
                <h3 id="54-另一种等价形式：log-p-elbo-kl">5.4 另一种等价形式：log p = ELBO + KL</h3>
                <p>还有一个很重要、但常被忽略的等价式：</p>
                <p>从一开始的：</p>
                <div class="math-block">$$
                \log p_\theta(x)
                = \mathcal{L}(\theta,\phi;x)
                + \mathrm{KL}\big(q_\phi(z\mid x)\,\|\,p_\theta(z\mid x)\big). \tag{5.10}
                $$</div>
                <p>这个式子可以从 (5.5) 再做一轮推导得到，这里直接给结论和直观解释：</p>
                <ul>
                <li>$\log p_\theta(x)$：固定不动（对 $\phi$ 来说是常数）；</li>
                <li>$\mathcal{L}(\theta,\phi;x)$：我们的优化目标；</li>
                <li>$\mathrm{KL}(q_\phi \| p_\theta)$：变分后验和真实后验的差距。</li>
                </ul>
                <p>因为 KL 总是非负的：</p>
                <div class="math-block">$$
                \mathrm{KL}\big(q_\phi \| p_\theta\big) \ge 0,
                $$</div>
                <p>所以：</p>
                <ul>
                <li>ELBO 是一个下界：$\mathcal{L} \le \log p_\theta(x)$；</li>
                <li>当 $q_\phi(z\mid x) = p_\theta(z\mid x)$ 时，KL = 0，<strong>下界恰好等于真实的 log-likelihood</strong>。</li>
                </ul>
                <blockquote>
                <p>这解释了「变分」这个名字：<br>我们在所有可选的 $q_\phi(z\mid x)$ 中做“变分”，<br>寻找一个让 ELBO 最大的 $q_\phi$，<br>也就是让 KL 最小、最接近真实后验的那一个。</p>
                </blockquote>
                <hr>
                </section><section class="post-section" id="6-vae-中的重参数化：为下一步做个铺垫"><h2 data-heading-id="6-vae-中的重参数化：为下一步做个铺垫">6. VAE 中的重参数化：为下一步做个铺垫</h2>
                <p>到这里，我们已经完成了 VAE 的<strong>核心数学结构</strong>：</p>
                <ol>
                <li>定义潜变量生成模型 $p_\theta(x,z)$；</li>
                <li>引入变分后验 $q_\phi(z\mid x)$；</li>
                <li>推到 ELBO：
                $$
                   \mathcal{L} = \mathbb{E}_{q_\phi}[\log p_\theta(x\mid z)]
                   - \mathrm{KL}(q_\phi \,\|\, p(z)).
                   $$</li>
                </ol>
                <p>接下来实际训练 VAE 时，会遇到一个关键问题：</p>
                <blockquote>
                <p><strong>这个 $\mathbb{E}_{q_\phi(z\mid x)}[\log p_\theta(x\mid z)]$ 怎么反向传播？</strong></p>
                </blockquote>
                <ul>
                <li>它是一个关于 $z$ 的期望；</li>
                <li>而 $z$ 又是从 $q_\phi(z\mid x)$ 里“随机采样”的；</li>
                <li>采样操作本身不可导。</li>
                </ul>
                <p>**重参数化技巧（reparameterization trick）**干的事就是：</p>
                <ul>
                <li>把采样 $z \sim q_\phi(z\mid x)$<br>改写成「先采一个固定分布的噪声 $\varepsilon$，再做一个可微变换 $z = g_\phi(\varepsilon,x)$」；</li>
                <li>这样整个 ELBO 就可以写成对 $\varepsilon$ 的期望：
                $$
                  \mathcal{L}(\theta,\phi;x)
                  = \mathbb{E}_{\varepsilon\sim p_0}
                  \big[\log p_\theta(x\mid g_\phi(\varepsilon,x))\big]
                  - \mathrm{KL}_\text{analytic},
                  $$
                然后就可以直接用普通的 backprop 来更新 $\theta,\phi$ 了。</li>
                </ul>
                <p>在高斯 VAE 中，这个重参数化具体就是那句经典的：</p>
                <div class="math-block">$$
                z = \mu_\phi(x) + \sigma_\phi(x)\odot \varepsilon,
                \quad \varepsilon \sim \mathcal{N}(0, I).
                $$</div>
                <hr>
                </section><section class="post-section" id="7-小结-下篇预告"><h2 data-heading-id="7-小结-下篇预告">7. 小结 &amp; 下篇预告</h2>
                <p>这一篇我们做了几件事：</p>
                <ol>
                <li>从「<strong>生成模型 vs 判别模型</strong>」开始，定位了 VAE 所属的方向；</li>
                <li>用潜变量模型的视角，写出了：<ul>
                <li>联合分布 $p_\theta(x,z)$；</li>
                <li>边缘似然 $\log p_\theta(x)$；</li>
                </ul>
                </li>
                <li>指出后验 $p_\theta(z\mid x)$ 的不可解性，动机地引出变分后验 $q_\phi(z\mid x)$；</li>
                <li>一步一步推到 VAE 的 ELBO，并给出了两种等价形式：<ul>
                <li>重构项 - KL 项；</li>
                <li>log p = ELBO + KL；</li>
                </ul>
                </li>
                <li>预告了 VAE 中的关键实现问题：<strong>期望上的梯度估计</strong>，也就是重参数化要解决的核心。</li>
                </ol>
                <blockquote>
                <p>在下一篇「生成模型系列（二）：VAE 中的重参数化与实现细节」里，我们会：</p>
                <ul>
                <li>站在 VAE 的角度，系统梳理重参数化技巧；</li>
                <li>写出高斯 VAE 中的完整数学推导；</li>
                <li>再给出一份带注释的 PyTorch 代码，把推导和代码一一对应起来。</li>
                </ul>
                </blockquote></section>
                </div>

                <div data-component="post-navigation"></div>
            </article>
        </div>

        <div data-include="../partials/sidebar-post.html"></div>
    </main>

    <div data-include="../partials/footer.html"></div>
    </div><!-- end page-container -->

    <script src="../script.js"></script>
</body>

</html>
