<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>概率和似然 | 慢变量</title>
    <link rel="stylesheet" href="../styles.css">
    <link rel="icon" href="../icons/deceleration.png" type="image/png">

    <script type="application/json" id="post-metadata">
        {
            "id": "11400",
            "slug": "probability-and-likelihood",
            "title": "概率和似然",
            "createdAt": "2025-12-03",
            "day": "3",
            "month": "Dec",
            "categories": [
                "数学研究"
            ],
            "tags": [
                "概率分布",
                "极大似然"
            ],
            "summary": [],
            "link": "posts/probability-and-likelihood.html"
        }
    </script>

    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$'], ['\[', '\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
</head>

<body>
    <div class="page-container">
        <div data-include="../partials/navbar.html"></div>

    <main class="main-content">
        <div class="wrapper">
            <article class="post">
                <div data-component="post-header"
                     data-day="3"
                     data-month="Dec"
                     data-title="概率和似然"
                     data-link=""
                     data-meta="&lt;span class=&quot;meta-item meta-categories&quot;&gt;分类：&lt;a href=&quot;#&quot;&gt;数学研究&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;meta-divider&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;meta-item meta-tags&quot;&gt;标签：&lt;a href=&quot;#&quot;&gt;概率分布&lt;/a&gt;,&lt;a href=&quot;#&quot;&gt;极大似然&lt;/a&gt;&lt;/span&gt;"
                     data-heading="h1"
                     data-meta-class="post-meta"
                     data-title-class="post-title"></div>

                <div class="post-content single-post-content">
                <p>在概率论中，概率（Probability）和似然（Likelihood）是两个相关但是完全不同的概念。理解它们的区别对于统计推断和机器学习等领域非常重要。</p>
                <section class="post-section" id="概率和似然"><h2 data-heading-id="概率和似然">概率和似然</h2>
                <ul>
                <li><p><strong>概率</strong>：描述的是<strong>在已知模型参数$\theta$的情况下，观察到某个数据$D$的可能性</strong>。例如：已知一枚硬币是公平的（这里的公平指的是正常的，没有被动过手脚的），那么抛一枚硬币，得到正面正面朝上的概率是就是$p(正面朝上|硬币公平)=0.5$。这里说的<u>模型的参数已知可以理解为投掷硬币的概率分布$P_\theta$是已知的</u>。概率的公式为：
                $$p(D|\theta),\theta=\begin{cases}0.5 &amp; D=正面朝上 \\ 0.5 &amp; D=反面朝上 \end{cases} \tag{1}$$
                其中，$D$表示观察到的数据，$\theta$表示模型的参数。那么，在已知$\theta$的情况下，我们可以计算抛10次硬币，得到7次正面朝上的概率为：
                $$p(D|\theta) = \binom{10}{7} (0.5)^7 (0.5)^3 \tag{2}$$</p>
                </li>
                <li><p><strong>似然</strong>：描述的是<strong>在已知数据$D$的情况下，模型参数$\theta$取某个值的可能性</strong>。例如：假设我们抛了一枚硬币10次，结果得到7次正面朝上，3次反面朝上。那么，我们想知道这枚硬币是公平的（$\theta=0.5$）的可能性有多大，或者偏向正面朝上的（$\theta&gt;0.5$）的可能性更大。计算的方式为：
                $$\mathcal{L}(\theta|7正面,3反面) = \binom{10}{7} \theta^7 (1-\theta)^3 \tag{3}$$</p>
                </li>
                </ul>
                <!-- more -->
                
                <p>从上面的公式(2)和(3)可以看出，概率和似然的数学表达式都可以写成：
                $$p(D|\theta) = \binom{10}{7} \theta^7 (1-\theta)^{3} \tag{4}$$</p>
                <p>因此，假设我们有一个概率模型，其参数用$\theta$表示，观测数据为：$D=\{x_1,x_2,\cdots,x_n\}$，假设观测数据之间相互独立，则似然可以定义为：
                $$\mathcal{L}(\theta|D) := p(D|\theta) = \prod_{i=1}^{n} p(x_i|\theta) \tag{5}$$</p>
                <p><strong>区别是概率是参数已知，数据未知，似然是观测数据已知，参数未知</strong>。要注意的是，从严格的概率论的角度来看，似然函数并不是真正的概率，它是在已知数据$D$的前提下，评估参数$\theta$的<strong>适应性（这个参数能不能很好的代表这个模型）</strong>。换句话说，似然函数衡量的是不同的参数$\theta$的取值对于观测数据$D$的解释能力。</p>
                </section><section class="post-section" id="极大似然估计"><h2 data-heading-id="极大似然估计">极大似然估计</h2>
                <p>那么该怎么提高评估参数的“适应性”，或者说找到一个参数$\hat{\theta}$，使得这个参数对于观测数据的解释能力最强呢？通俗来说，就是找到一个参数$\hat{\theta}$，使得观测数据在这个以$\hat{\theta}$为参数的模型下出现的概率最大，即：
                $$
                \hat{\theta} = \arg\max_{\theta} \mathcal{L}(\theta|D)=\arg\max_{\theta} p(D|\theta) \tag{6}
                $$</p>
                <p>这就是<strong>极大似然估计(Maximum Likehood Estimation, MLE)</strong>，它是一种经典的参数估计方法，其目标是找到参数$\hat{\theta}$使得似然函数最大。</p>
                <p>在实际使用中，我们通常用对数似然函数来替换原始的似然函数，这是因为概率值通常很小，直接计算似然函数可能会导致数值下溢（Underflow），对数似然的定义如下：
                $$
                \log \mathcal{L}(\theta|D)=\log p(D|\theta)=\log\prod_{i=1}^{n}p(x_i|\theta)=\sum_{i=1}^{n}\log p(x_i|\theta) \tag{7}
                $$
                则对数似然的极大化问题可以表示为：
                $$
                \hat{\theta} = \arg\max_{\theta} \log \mathcal{L}(\theta|D)=\arg\max_{\theta} \sum_{i=1}^{n}\log p(x_i|\theta) \tag{8}
                $$</p>
                <p>对数似然函数的优点如下：</p>
                <ol>
                <li>将乘法转换为加法，避免数值下溢</li>
                <li>对数函数是单调递增的，所以最大化对数似然等价于最大化原始的似然函数，对于优化结果没有影响</li>
                <li>便于计算梯度和优化</li>
                </ol>
                <p>如果我们在式(8)的右边除以$n$，则可以得到一个更加紧凑的形式：
                $$
                \begin{aligned}
                \hat{\theta} &amp;= \arg\max_{\theta} \frac{1}{n} \sum_{i=1}^{n}\log p(x_i|\theta) \\
                &amp;= \arg\max_{\theta} \mathbb{E}_{x \sim p_{data}}\left[\log p(x|\theta)\right]
                \end{aligned}  \tag{9}
                $$</p>
                <p>对于式(9)，如果我们在最大化的目标前面加一个负号，将其转化为一个最小化问题，可以得到：
                $$
                \begin{aligned}
                \hat{\theta} &amp;= \arg\min_{\theta} -\mathbb{E}_{x \sim p_{data}}\left[\log p(x|\theta)\right] \\
                &amp;= \arg\min_{\theta} - \sum_{x} p(x)\log p(x|\theta)
                \end{aligned}  \tag{10} 
                $$
                式(10)中的目标函数实际上是<strong>交叉熵（Cross-Entropy）</strong>，它衡量了真实数据分布$p_{data}$和模型分布$p(x|\theta)$之间的差异。因此，<span style="color:blue;">极大似然估计等价于最小化交叉熵</span>，这也是机器学习中常用的损失函数之一。</p></section>
                </div>

                <div data-component="post-navigation"></div>
            </article>
        </div>

        <div data-include="../partials/sidebar-post.html"></div>
    </main>

    <div data-include="../partials/footer.html"></div>
    </div><!-- end page-container -->

    <script src="../script.js"></script>
</body>

</html>
