{
  "generatedAt": "2025-11-06T10:48:06.569Z",
  "postCount": 14,
  "posts": [
    {
      "id": "11400",
      "slug": "11400",
      "title": "表格展示测试",
      "createdAt": "2025-11-05",
      "day": "5",
      "month": "Nov",
      "categories": [
        "数据实践"
      ],
      "tags": [
        "表格",
        "样式",
        "演示"
      ],
      "summary": [
        {
          "type": "p",
          "html": "本文用于检查表格在最新样式下的表现，包括标题行背景、隔行底色、悬停高亮等效果。"
        },
        {
          "type": "h2",
          "html": "模型表现概览 <span class=\"section-anchor\">#</span>"
        },
        {
          "type": "p",
          "html": "下表列出了四个候选模型在验证集上的核心指标："
        },
        {
          "type": "table",
          "html": "<table class=\"latex-table\"><caption>表 1：不同模型的关键性能对比</caption>\r\n                            <thead>\r\n                                <tr>\r\n                                    <th>模型</th>\r\n                                    <th>参数量（M）</th>\r\n                                    <th>准确率</th>\r\n                                    <th>召回率</th>\r\n                                    <th>F1 分数</th>\r\n                                </tr>\r\n                            </thead>\r\n                            <tbody>\r\n                                <tr>\r\n                                    <td>Baseline-CNN</td>\r\n                                    <td>28.6</td>\r\n                                    <td>0.871</td>\r\n                                    <td>0.842</td>\r\n                                    <td>0.856</td>\r\n                                </tr>\r\n                                <tr>\r\n                                    <td>Hybrid-Transformer</td>\r\n                                    <td>82.4</td>\r\n                                    <td>0.906</td>\r\n                                    <td>0.897</td>\r\n                                    <td>0.901</td>\r\n                                </tr>\r\n                                <tr>\r\n                                    <td>LightSeq</td>\r\n                                    <td>18.2</td>\r\n                                    <td>0.861</td>\r\n                                    <td>0.884</td>\r\n                                    <td>0.872</td>\r\n                                </tr>\r\n                                <tr>\r\n                                    <td>AutoML-Search</td>\r\n                                    <td>145.7</td>\r\n                                    <td>0.918</td>\r\n                                    <td>0.905</td>\r\n                                    <td>0.911</td>\r\n                                </tr>\r\n                            </tbody></table>"
        }
      ],
      "link": "posts/11400.html",
      "metaText": "2025-11-05 | 分类：数据实践 | 标签：表格， 样式， 演示"
    },
    {
      "id": "11398",
      "slug": "11398",
      "title": "对偶梯度下降的数值实验",
      "createdAt": "2025-11-04",
      "day": "4",
      "month": "Nov",
      "categories": [
        "机器学习"
      ],
      "tags": [
        "对偶梯度",
        "实验"
      ],
      "summary": [
        {
          "type": "p",
          "html": "我们在三个凸优化基准上考察对偶梯度下降的表现：线性规划、谱范数约束问题以及核范数正则化问题。实验表明，合理的初始对偶变量能显著缩短收敛时间。"
        }
      ],
      "link": "posts/11398.html",
      "metaText": "2025-11-04 | 分类：机器学习 | 标签：对偶梯度， 实验"
    },
    {
      "id": "11399",
      "slug": "11399",
      "title": "代码块渲染测试",
      "createdAt": "2025-11-04",
      "day": "4",
      "month": "Nov",
      "categories": [
        "数学研究"
      ],
      "tags": [
        "代码",
        "渲染",
        "测试"
      ],
      "summary": [
        {
          "type": "p",
          "html": "本文用于确认首页摘要与正文的代码块在复制按钮、语法高亮、背景、边距等方面与参考博客保持一致。"
        },
        {
          "type": "h2",
          "html": "Python 示例 <span class=\"section-anchor\">#</span>"
        },
        {
          "type": "p",
          "html": "以下示例展示了一个包含注释、多行语句与 f-string 的 Python 代码块："
        },
        {
          "type": "pre",
          "html": "<pre><code class=\"language-python\">from pathlib import Path\r\n\r\n# 加载数据并统计行数\r\ndef count_lines(paths: list[Path]) -> dict[str, int]:\r\n    summary = {}\r\n    for path in paths:\r\n        text = path.read_text(encoding=\"utf-8\")\r\n        summary[path.name] = len(text.splitlines())\r\n    return summary\r\n\r\nif __name__ == \"__main__\":\r\n    stats = count_lines([Path(\"notes.md\"), Path(\"todo.md\")])\r\n    for name, lines in stats.items():\r\n        print(f\"{name}: {lines} 行\")\r\n</code></pre>"
        }
      ],
      "link": "posts/11399.html",
      "metaText": "2025-11-04 | 分类：数学研究 | 标签：代码， 渲染， 测试"
    },
    {
      "id": "11388",
      "slug": "11388",
      "title": "流形上的最速下降：5. 对偶梯度下降",
      "createdAt": "2025-11-03",
      "day": "3",
      "month": "Nov",
      "categories": [
        "数学研究"
      ],
      "tags": [
        "矩阵",
        "优化器",
        "Muon",
        "约束",
        "最速下降"
      ],
      "summary": [
        {
          "type": "p",
          "html": "前四篇文章我们求解了几个具体的、给参数添加等式约束的最速下降问题，其中第三、四篇的问题难以求得解析解，于是我们提出了相应的不动点迭代法。本篇文章聚焦于 Jeremy\r\n                        Bernstein 在《Modular Manifolds》中提出的“对偶梯度下降（Dual Gradient Descent）”解法，尝试把它融入我们此前搭建的理论框架中。"
        },
        {
          "type": "h2",
          "html": "基本概念 <span class=\"section-anchor\">#</span>"
        },
        {
          "type": "p",
          "html": "Jeremy Bernstein 的解法，发表在 Thinking Machines Lab 的博客《Modular Manifolds》中，文中称其为“对偶上升（Dual\r\n                            Ascent）”。这里我们沿用“对偶梯度下降”的命名，以保持与前几篇文章的风格一致。设 $\\boldsymbol{W} \\in \\mathbb{R}^{n \\times\r\n                            m}$、$\\boldsymbol{G} \\in \\mathbb{R}^{n \\times m}$，我们记谱范数为 $\\Vert\\boldsymbol{G}\\Vert_2$，核范数为\r\n                            $\\Vert\\boldsymbol{G}\\Vert_*$。"
        },
        {
          "type": "p",
          "html": "$$\\nabla_{\\boldsymbol{G}}\\Vert\\boldsymbol{G}\\Vert_* = \\sum_i \\boldsymbol{u}_i\r\n                            \\boldsymbol{v}_i^{\\top} = \\boldsymbol{U}\\boldsymbol{V}^{\\top} =\r\n                            \\operatorname{msign}(\\boldsymbol{G})$$"
        },
        {
          "type": "p",
          "html": "也就是说，核范数的梯度恰好是 $\\operatorname{msign}$ 算子。这一点将在后续的对偶推导中扮演核心角色。与此同时，我们也会频繁用到\r\n                            $\\mathrm{tr}(\\cdot)$、$\\operatorname{sign}(\\cdot)$ 等符号，因此建议读者先回顾前文中的符号约定。"
        }
      ],
      "link": "posts/11388.html",
      "metaText": "2025-11-03 | 分类：数学研究 | 标签：矩阵， 优化器， Muon， 约束， 最速下降"
    },
    {
      "id": "11397",
      "slug": "11397",
      "title": "对偶变量的收缩映射证明",
      "createdAt": "2025-11-02",
      "day": "2",
      "month": "Nov",
      "categories": [
        "数学研究"
      ],
      "tags": [
        "收缩映射",
        "对偶变量"
      ],
      "summary": [
        {
          "type": "p",
          "html": "证明对偶迭代 $\\\\lambda_{t+1} = T(\\\\lambda_t)$ 收敛的常用方法是展示 $T$ 为收缩映射。若存在 $0 < \\kappa < 1$ 使得"
        }
      ],
      "link": "posts/11397.html",
      "metaText": "2025-11-02 | 分类：数学研究 | 标签：收缩映射， 对偶变量"
    },
    {
      "id": "11396",
      "slug": "11396",
      "title": "Stiefel 流形的投影梯度法",
      "createdAt": "2025-10-28",
      "day": "28",
      "month": "Oct",
      "categories": [
        "数学研究"
      ],
      "tags": [
        "Stiefel",
        "梯度法"
      ],
      "summary": [
        {
          "type": "p",
          "html": "Stiefel 流形 $\\\\mathrm{St}(n, k)$ 定义为所有 $n \\times k$ 正交矩阵的集合，满足 $X^{\\\\top}X =\r\n                        I_k$。对任意基于梯度的更新，我们需要将结果重新投影回流形。本文推导了基于 Cayley 变换的二阶近似投影："
        }
      ],
      "link": "posts/11396.html",
      "metaText": "2025-10-28 | 分类：数学研究 | 标签：Stiefel， 梯度法"
    },
    {
      "id": "11385",
      "slug": "11385",
      "title": "低精度 Attention 可能存在有偏的舍入误差",
      "createdAt": "2025-10-27",
      "day": "27",
      "month": "Oct",
      "categories": [
        "信息时代"
      ],
      "tags": [
        "近似",
        "分析",
        "优化",
        "Attention",
        "低精度"
      ],
      "summary": [
        {
          "type": "p",
          "html": "低精度训练是把 Transformer 模型推向大规模的关键工程手段，但一旦计算链路中出现系统性的舍入偏差，整个网络就可能以极快的速度漂移。最近的论文把目光聚焦到\r\n                        Attention 的核心算子上，指出 bias 并非出在分块策略，而是出在精度本身。"
        },
        {
          "type": "h2",
          "html": "背景动机 <span class=\"section-anchor\">#</span>"
        },
        {
          "type": "p",
          "html": "在 GPT 类模型中，我们习惯把激活、梯度乃至优化器状态都压到 8 位甚至 4 位。过去人们认为标准化层和 Softmax\r\n                            的鲁棒性足够兜住误差，但实际部署常常在第二层左右就爆炸。作者通过对比 float16 与 int8+scale 的训练轨迹，展示了清晰的漂移趋势。"
        }
      ],
      "link": "posts/11385.html",
      "metaText": "2025-10-27 | 分类：信息时代 | 标签：近似， 分析， 优化， Attention， 低精度"
    },
    {
      "id": "11395",
      "slug": "11395",
      "title": "约束神经网络的奇异值调控",
      "createdAt": "2025-10-20",
      "day": "20",
      "month": "Oct",
      "categories": [
        "机器学习"
      ],
      "tags": [
        "神经网络",
        "奇异值",
        "正则化"
      ],
      "summary": [
        {
          "type": "p",
          "html": "奇异值调控通过规范化每一层的谱范数，防止梯度爆炸。我们提出了指数移动目标值 $\\gamma$，令最大奇异值逐步靠近设定常数。"
        }
      ],
      "link": "posts/11395.html",
      "metaText": "2025-10-20 | 分类：机器学习 | 标签：神经网络， 奇异值， 正则化"
    },
    {
      "id": "11394",
      "slug": "11394",
      "title": "对偶梯度下降的学习率策略",
      "createdAt": "2025-09-15",
      "day": "15",
      "month": "Sep",
      "categories": [
        "机器学习"
      ],
      "tags": [
        "对偶梯度",
        "学习率"
      ],
      "summary": [
        {
          "type": "p",
          "html": "对偶梯度下降的更新式为"
        }
      ],
      "link": "posts/11394.html",
      "metaText": "2025-09-15 | 分类：机器学习 | 标签：对偶梯度， 学习率"
    },
    {
      "id": "11393",
      "slug": "11393",
      "title": "约束优化的对偶间隙分析",
      "createdAt": "2025-08-30",
      "day": "30",
      "month": "Aug",
      "categories": [
        "数学研究"
      ],
      "tags": [
        "对偶间隙",
        "优化理论"
      ],
      "summary": [
        {
          "type": "p",
          "html": "对偶间隙是评价约束优化算法收敛性的关键指标。若原始问题为"
        },
        {
          "type": "p",
          "html": "$$\\min_{x \\in \\mathcal{X}} f(x) \\quad \\text{s.t.} \\quad h_i(x) \\le 0,$$"
        },
        {
          "type": "p",
          "html": "其拉格朗日对偶为"
        },
        {
          "type": "p",
          "html": "$$\\max_{\\lambda \\ge 0} g(\\lambda) = \\inf_{x \\in \\mathcal{X}} L(x, \\lambda).$$"
        },
        {
          "type": "p",
          "html": "本文展示了如何在线估计 $g(\\lambda)$，从而提供精确的停止准则。"
        }
      ],
      "link": "posts/11393.html",
      "metaText": "2025-08-30 | 分类：数学研究 | 标签：对偶间隙， 优化理论"
    },
    {
      "id": "11392",
      "slug": "11392",
      "title": "Muon 优化的动量调度",
      "createdAt": "2025-07-12",
      "day": "12",
      "month": "Jul",
      "categories": [
        "机器学习"
      ],
      "tags": [
        "Muon",
        "优化器",
        "动量"
      ],
      "summary": [
        {
          "type": "p",
          "html": "Muon 优化器通过引入对偶变量平衡权值范数与梯度范数。为了进一步提升训练稳定性，我们提出引入时间相关的动量系数 $\\\\beta_t$："
        },
        {
          "type": "p",
          "html": "$$m_t = \\\\beta_t m_{t-1} + (1-\\\\beta_t) g_t,$$"
        },
        {
          "type": "p",
          "html": "其中 $g_t$ 为瞬时梯度。本文给出了 $\\\\beta_t$ 的自适应调节策略：$\\\\beta_t = 1 - \\frac{1}{(t +\r\n                        t_0)^\\alpha}$，并在语言模型预训练中验证了其收益。"
        }
      ],
      "link": "posts/11392.html",
      "metaText": "2025-07-12 | 分类：机器学习 | 标签：Muon， 优化器， 动量"
    },
    {
      "id": "11391",
      "slug": "11391",
      "title": "谱球面上的投影算子",
      "createdAt": "2025-06-01",
      "day": "1",
      "month": "Jun",
      "categories": [
        "数学研究"
      ],
      "tags": [
        "谱球面",
        "投影",
        "优化"
      ],
      "summary": [
        {
          "type": "p",
          "html": "谱球面上的投影算子在优化中非常常用。设矩阵 $Y$ 的奇异值分解为 $U \\Sigma V^{\\top}$，其中 $\\Sigma = \\operatorname{diag}(\\sigma_1,\r\n                        \\ldots, \\sigma_n)$。为了得到 $\\Pi_{\\mathcal{S}}(Y)$，我们只需把最大奇异值归一化："
        },
        {
          "type": "p",
          "html": "$$\\Pi_{\\mathcal{S}}(Y) = \\frac{Y}{\\sigma_1}.$$"
        },
        {
          "type": "p",
          "html": "然而，当 $\\sigma_1$ 很小或存在数值噪声时，直接归一化会导致不稳定。本文提出了一个软阈值版本："
        },
        {
          "type": "p",
          "html": "$$\\Pi_{\\mathcal{S}}^{\\epsilon}(Y) = \\frac{Y}{\\max(\\sigma_1, \\epsilon)}$$"
        },
        {
          "type": "p",
          "html": "通过选取合适的 $\\epsilon$，我们可以同时保证收敛性与稳定性。"
        }
      ],
      "link": "posts/11391.html",
      "metaText": "2025-06-01 | 分类：数学研究 | 标签：谱球面， 投影， 优化"
    },
    {
      "id": "11390",
      "slug": "11390",
      "title": "随机矩阵的谱范数估计",
      "createdAt": "2025-05-13",
      "day": "13",
      "month": "May",
      "categories": [
        "数学研究"
      ],
      "tags": [
        "随机矩阵",
        "谱范数"
      ],
      "summary": [
        {
          "type": "p",
          "html": "设有随机矩阵 $A \\in \\mathbb{R}^{n \\times\r\n                        n}$，我们希望在不进行完整奇异值分解的情况下估计其谱范数。随机化幂法提供了一种低成本的方式，仅需重复乘以随机向量即可逼近最大奇异值。"
        },
        {
          "type": "p",
          "html": "具体而言，我们初始化单位向量 $x_0$，迭代 $x_{k+1} = \\frac{Ax_k}{\\|Ax_k\\|_2}$。当迭代收敛时，$\\|Ax_k\\|_2$\r\n                        即为谱范数估计。本文给出了迭代误差上界，并将其与高概率界联系起来。"
        },
        {
          "type": "p",
          "html": "实验部分，我们展示了该方法在 Muon 训练统计中的应用，其中谱范数估计用于调节学习率。实践表明，该估计的误差通常在 5% 以内。"
        }
      ],
      "link": "posts/11390.html",
      "metaText": "2025-05-13 | 分类：数学研究 | 标签：随机矩阵， 谱范数"
    },
    {
      "id": "11389",
      "slug": "11389",
      "title": "流形上的最速下降：6. 拉格朗日重参数化",
      "createdAt": "2025-04-01",
      "day": "1",
      "month": "Apr",
      "categories": [
        "数学研究"
      ],
      "tags": [
        "最速下降",
        "拉格朗日",
        "优化"
      ],
      "summary": [
        {
          "type": "p",
          "html": "为了把约束优化问题转化为无约束形式，我们通常会引入拉格朗日乘子。然而在流形优化中，更高效的方式是直接重参数化目标变量。设 $\\theta = f(\\phi)$，其中 $f$\r\n                        满足约束。则原问题"
        },
        {
          "type": "p",
          "html": "$$\\min_{\\theta \\in \\mathcal{M}} J(\\theta)$$"
        },
        {
          "type": "p",
          "html": "转化为"
        },
        {
          "type": "p",
          "html": "$$\\min_{\\phi} J(f(\\phi)).$$"
        },
        {
          "type": "p",
          "html": "由于 $f$ 保证了 $\\theta$ 始终位于流形 $\\mathcal{M}$ 上，我们可以直接对 $\\phi$ 应用最速下降法。"
        },
        {
          "type": "p",
          "html": "重参数化的梯度可由链式法则得到：$$\\nabla_{\\phi} J(f(\\phi)) = (\\nabla_{\\theta} J)^{\\top} \\cdot J_f(\\phi)$$，其中 $J_f$ 是雅可比矩阵。这种写法不仅避免了显式投影，还能让学习率调节更加稳定。"
        }
      ],
      "link": "posts/11389.html",
      "metaText": "2025-04-01 | 分类：数学研究 | 标签：最速下降， 拉格朗日， 优化"
    }
  ]
}
