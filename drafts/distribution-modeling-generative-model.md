---
title: "从分布建模的角度看生成模型"
createdAt: 2025-12-31
categories:
  - 人工智能
tags:
  - 生成模型
  - 分布建模
---

**文章来源**：[一图读懂生成式模型 (Generative Modeling) 的主要流派](https://zhuanlan.zhihu.com/p/1905427654582183119)

生成模型的终极目标，是去建模真实世界数据的复杂联合分布 $p_{\text{data}}(x)$。一旦学到了这个分布，我们就能从中采样，生成“看起来像真的”新样本。

举个直观例子：如果我们想生成猫咪图片，真实世界中的猫千姿百态——颜色、姿态、背景、光照都不同——这让 $p_{\text{data}}(x)$ 变得异常复杂。生成模型要做的，就是尽可能捕捉这种复杂性，然后从学到的分布里采样出新的猫咪图片。

但问题是：**直接学习高维联合分布通常极其困难**。因此主流生成建模方法，大多可以归结为三条路线：

- **分布分解建模**：把联合分布拆成一串条件分布来学（链式法则）
- **分布变换建模**：从一个简单分布出发，通过映射/路径“变换”成数据分布
- **混合建模**：把不同路线的优势组合起来，兼顾质量与效率

<!-- more -->

## 分布分解建模（Distribution Factorization Modeling）

这条路线的核心思想是：利用概率论的链式法则，把高维联合分布分解为条件分布的乘积。对多维数据 $x=(x_1,x_2,\ldots,x_n)$，有：

$$
\begin{aligned}
p(x) = p(x_1, x_2, \ldots, x_n)
&= p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)\cdots \\
&= \prod_{i=1}^{n} p(x_i \mid x_1, x_2, \ldots, x_{i-1})
\end{aligned}
\tag{1}
$$

这类方法最典型的代表，就是**自回归模型**（Autoregressive Models, AR）。AR的生成过程是严格序列化的：每一步根据已生成的部分，预测下一个“单元”。这个单元可以是像素、token，或更抽象的表示。

> 直觉上：AR 把“生成”变成了一个很长的条件概率链，每一环都尽可能学得准确。

它的优势在于：能够细致地建模复杂依赖关系，因此常能生成高质量样本；代价在于：**生成往往难以并行**，速度可能受限。

根据要预测的下一个“单元”不同，自回归模型常见可分为：

- **Next-Pixel Prediction**  
  典型如 PixelRNN / PixelCNN：直接在像素空间逐像素生成图像。

- **Next-Token Prediction**  
  最经典的 AR 形式：广泛用于大语言模型（如 GPT 系列），也用于早期的图像/视频 token 化生成（例如一些基于离散 token 的方法）。

- **Next-Set-of-Tokens Prediction**  
  一次预测一组 token，而不是单个 token：在尽量保持质量的同时提升并行度，代表性如 MaskGIT 思路。

- **Next-Scale Prediction**  
  以多尺度方式逐步生成（先低分辨率/全局，再高分辨率/细节），用于同时捕捉全局结构与局部纹理（你提到的 VAR 等思路可归在这一类讨论语境中）。

- **Next-Embedding Prediction**  
  不在像素或离散 token 上做一步步预测，而是直接在 latent/embedding 空间里做 AR 式建模。一个常见动机是：latent 往往更“紧凑/语义化”，也更利于统一多模态输入输出的表示形式。


## 分布变换建模（Distribution Mapping Modeling）

与分解联合分布不同，这条路线的核心是：学习一个映射$G$，把易采样的简单分布$p_Z(z)$（如高斯/均匀）变换为目标数据分布$p_X(x)$：

- 先采样$z \sim p_Z(z)$
- 再通过$x = G(z)$得到样本

这一路线下面通常可以再分成两大分支。

### 直接映射建模：一步到位的$x = G(z)$
- **GAN（Generative Adversarial Networks）**  
  通过生成器 $G$ 与判别器 $D$ 的对抗训练，让 $G$ 逐渐学会把噪声映射成“像真数据”的样本。

- **VAE（Variational Autoencoders）**  
  通过潜变量与变分推断，学习从潜在空间到数据空间的生成过程；训练目标通常对应证据下界（ELBO）。直观上，它同时学“如何把数据压到潜空间”与“如何从潜空间还原回数据空间”。

### 概率路径建模
这类方法不追求“一步到位”的映射，而是构造一个连续或离散的演化过程，让样本从简单初始分布逐渐变成目标分布。

从某种意义上，它也可以被理解为在“时间/步数维度”上建模条件分布，因此与“分解建模”的思想存在呼应。

- **扩散模型（Diffusion Models）**  
  正向过程逐渐加噪，把数据推向纯噪声；反向过程学习逐步去噪，把噪声还原为数据。每一步都涉及条件分布的学习，因此常被描述为马尔可夫链式的逐步生成。

- **流匹配 / 连续流模型（Flow Matching Models）**  
  通过定义连续时间的演化过程，学习把简单分布“运输”到数据分布。实现形式可以是离散化步进，也可以是连续 ODE/flow 的训练视角。

## 混合建模（Hybrid Modeling）

近年来，一个越来越清晰的趋势是：**把AR与Diffusion（或更广义的路径式生成）结合**，试图同时拿到：
- AR在依赖关系建模上的优势
- Diffusion在样本质量与分布覆盖上的优势

### 为什么要混合？
- **优势互补**：AR擅长建模元素之间的依赖结构；Diffusion往往更擅长生成高保真样本。
- **统一多模态**：文本侧AR是主流范式，视觉侧Diffusion是主流范式。若目标是原生统一多模态大模型，混合路线提供了一条工程上与方法论上都很自然的路径。

### 四类常见的结合方式

| 类别                                      | 核心做法                                          | 代表性工作（示例）               | 直观备注                                                      |
| ----------------------------------------- | ------------------------------------------------- | -------------------------------- | ------------------------------------------------------------- |
| 1) 共享参数的AR + Diffusion               | 同一套backbone/参数同时做AR与Diffusion任务        | Transfusion                      | 形式统一更容易；但两类目标差异大时可能互相牵制                |
| 2) AR backbone + Diffusion loss / sampler | 以AR为主，加入diffusion式目标或轻量diffusion head | MAR                              | 常见叙事：AR建模依赖结构，Diffusion更像刻画每个元素的局部分布 |
| 3) 解耦的两阶段（AR→Diffusion）           | 先AR生成条件表示，再一次性交给强Diffusion解码生成 | EMU2                             | 流程更清晰：AR负责规划/条件，Diffusion 负责渲染/解码          |
| 4) 自回归扩散（AR inside Diffusion）      | 把自回归的约束/顺序性融进Diffusion本身            | Diffusion Forcing / CausalFusion | 本质仍是Diffusion框架，但依赖关系在Diffusion内部处理          |


## 小结：

- **分解建模（AR）**：把联合分布拆成条件链来学，依赖结构强，但生成并行性常受限  
- **变换建模（GAN/VAE/扩散/流）**：从简单分布出发，通过映射或路径得到目标分布  
- **混合建模**：在“结构建模能力”和“高质量采样能力”之间折中，尤其契合多模态统一的大趋势
