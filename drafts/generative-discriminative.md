---
title: "生成式和判别式模型"
createdAt: 2025-12-06
categories:
  - 人工智能
tags:
  - 有监督学习
  - 无监督学习
  - 生成模型
  - 判别模型
---

在AI中，「学习」总是离不开数据，一个朴素但是真实的假设是：**存在一个真实但未知的分布$p^{\ast}$**，我们所能接触到的数据，都是从这个分布中采样得到的。
## 数据形态与预测目标

针对采集到的数据的情况不同（有没有标签），我们一般有两种不同的学习方式：

- **有监督学习**: 数据通常是以「输入-标签」对$(x,y)$的形式存在，我们假设它们服从联合分布：
$$
(x, y) \sim p(x, y) \tag{1}
$$

- **无监督学习**: 数据只有「输入」$x$的，我们假设它们服从边缘分布：
$$
x \sim p(x) \tag{2}
$$

## 有监督学习中的判别与生成
在有监督学习中最常见的任务是：**给定输入$x$，预测标签$y$**。如果站在概率论的角度，最自然的决策规则就是**最大后验概率（MAP）**：
$$
\hat{y}(x) = \arg\max_y p^{\ast}(y|x) \tag{3}
$$

<!-- more -->

这个规则的直观意义是：
> [cite] 看到输入$x$后，我们选择最有可能的输出$y$作为预测结果。

但是现在的问题是：**我们并不知道$p(y|x)$是什么**，只能通过有限的数据去训练一个概率分布模型$p_\theta$去近似它：

(1) 既然我们的目标是$p^{\ast}(y|x)$，那就直接去建模$p(y|x)$：
$$
p(y|x) \approx p_\theta(y|x) \tag{3}
$$
当训练好模型$p_\theta(y|x)$之后，我们就可以选择那个概率最大的$y$作为预测结果：
$$
\hat{y}(x) = \arg\max_y p_\theta(y|x) \tag{4}
$$

为了训练这个模型，假设有一个训练集$\{(x_i, y_i)\}_{i=1}^N$，其中$x_i$是输入，$y_i$是对应的标签，则可以使用极大似然估计来优化：
$$
\theta^\ast = \arg\max_\theta \prod_{i=1}^N p_{\theta}(y_i|x_i) \tag{5}
$$
为了方便表示和计算，我们通常会对似然函数取对数，将乘积转化为求和，然后在前面加上负号，将最大化问题转化为最小化问题，得到最终的目标函数：
$$\theta^\ast = \arg\min_\theta -\sum_{i=1}^N \log p_\theta(y_i|x_i) \tag{6}$$ 

这就是**判别式模型（discriminative model）**，它直接关注于输入和输出之间的关系。常见的判别式模型包括逻辑回归、支持向量机（SVM）、条件随机场（CRF）以及大多数深度学习模型（如分类神经网络）。

(2) **生成式模型（Generative Model）** 选择了另外一条路看似更加曲折的路来建模$p(y|x)$。它认为，只有理解了数据的产生机制，才能做出更好的判断。所以生成式模型是去建模联合分布$p_\theta(x, y)$，通过贝叶斯链式法则可以分解为:
$$
p_\theta(x, y) = p_\theta(x|y)p_\theta(y) \tag{7}
$$
这对应了生成的物理过程，先选择一个类别$y$，然后再根据这个类别生成具体的数据$x$。为了直接表示$p(y|x)$，我们可以使用贝叶斯公式：
$$
\begin{aligned}
p_\theta(y|x) &= \frac{p_\theta(x, y)}{p_\theta(x)} \\
&=\frac{p_{\theta}(x|y)p_\theta(y)}{\sum_{y'}p_{\theta}(x|y')p_\theta(y')}
\end{aligned}
\tag{8}
$$

在预测时，由于分母$p_\theta(x)$对所有的$y$都是一样的，所以我们只需要选择那个使得分子最大的$y$作为预测结果：
$$
\hat{y}(x) = \arg\max_y p_\theta(x|y)p_\theta(y) \tag{9}
$$

训练过程依旧是极大似然估计，但是这次是去最大化联合分布的似然函数：
$$
\theta^\ast = \arg\max_\theta \prod_{i=1}^N p_{\theta}(x_i, y_i) = \arg\max_\theta \prod_{i=1}^N p_{\theta}(x_i|y_i)p_{\theta}(y_i) \tag{10}
$$

同样地，为了方便表示和计算，我们通常会对似然函数取对数，将乘积转化为求和，然后在前面加上负号，将最大化问题转化为最小化问题，得到最终的目标函数：
$$
\theta^\ast = \arg\min_\theta -\sum_{i=1}^N \left[\log p_\theta(x_i|y_i) + \log p_\theta(y_i) \right] \tag{11}
$$


## 无监督学习中的判别与生成
在无监督学习中，数据通常只有输入$x$，没有对应的标签$y$。此时，$p(y|x)$的判别任务不存在了，最自然的目标是**直接建模$p(x)$**，即**理解数据的结构**：
$$p(x) \approx p_\theta(x) \tag{12}$$
通过最大化对数边际似然来学习模型参数：
$$\theta^\ast = \arg\min_\theta - \mathbb{E}_{x\sim p(x)}[\log p_\theta(x)] \tag{13}$$

但是直接学习$p(x)$的问题有很大的问题，我们可以从两个层面来理解这个问题：
### 1. 数学层面
假设$x$是一张MINST中的$28\times 28$大小的灰度图，每个像素的取值有$0\sim 255$共256中可能，那么总共可能的$x$的个数就有$256^{28\times 28} = 256^{784}$个，这是一个大到无法想象的数量级。

就算我们可以收集到10亿张图，放到这个空间里都是极其稀疏的，我们要填满这个空间来准确估计$p(x)$需要$256^{784}$个样本，这是一个无法实际实现的数量级。所以在现实中经常会引入极强的假设，例如假设$p(x)$是一个高维的高斯分布，那么我们就只需要$28\times 28 = 784$个参数就可以完全描述这个分布。但是真实数据的分布通常是复杂的，无法用一个简单的模型来近似。一旦假设错了，那学到的$p_\theta(x)$也就没什么用。

### 2. 几何层面
虽然$x$看起来是在$28\times 28$的空间里，但是真实数据其实分布在一个**极低维的流行**上，例如随便生成一堆随机的像素点，它们的组合看起来像手写体数字的概率几乎为$0$。真正的手写体数字只占据了像素空间中极小，极薄的「一层皮」。如果我们在原始空间直接建模$p(x)$，比如使用高斯混合模型，就会发现模型倾向于覆盖整个像素空间，导致生成的图像是模糊的平均值，因为**模型试图去填满那些本来就不该有数据的空隙**。

这也是为什么VAE，GAN等模型要引入一个中间的**隐变量$z$**，因为它们承认直接建模$p(x)$太难了，所以它们：
1. 先假设一个简单的低维分布$p(z)$，比如VAE中假设的高斯分布。
2. 然后学习一个映射$x=g(z)$，也就是我们VAE中的解码器或者GAN中生成器。

本质上，VAE和GAN实在学习容易得到的一个低维分布到复杂高维的真实数据分布之间的映射，而不是直接估计真实数据分布。